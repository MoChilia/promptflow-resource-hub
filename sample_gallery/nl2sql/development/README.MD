# Developing Natural Language to SQL Converter

In this flow, we show how to develop a Natural Language to SQL Converter using Azure OpenAI GPT-4 model and PromptFlow. The implementation is loosely based on the [LangChain SQL Database Agent](https://python.langchain.com/docs/use_cases/sql/quickstart)

#### Flow Overview:

- Gather Inputs:
  - user query (str): Natural language question from user
  - schema name (str): Name of the schema to run the queries against. e.g - dbo, salesLT
- (Python) Get Table Names:
  - Query the database for a list of all table names in the specified schema
- (LLM) Identify Table Names:
  - Use GPT-4 to list the required tables to query based on the user query. This is to ensure only the schema of relevant tables is passed in context to generate the SQL Query
- (Python) Get Table Schemas:
  - Query the database to get the structure and a few samples rows of the identified tables
  - We use the [SQLDatabase](https://python.langchain.com/docs/integrations/tools/sql_database) class from Langchain to gather the table info
- (LLM) Generate SQL Query:
  - Pass the tables info with row sampling as context and the user question to GPT-4 to generate a SQL Query
  - Prompt used - [Generate SQL Prompt](./generate_sql_query.jinja2)
- (LLM) Validate Generated SQL Query:

  - Validate the SQL query for completeness and generate a detailed explaination of the SQL query
  - Prompt used - [Validate SQL Prompt](./validate_sql_query.jinja2)

  _Note:_ Prompts in this sample are specific to generate T-SQL complaint queries for [AdventureWorksLT](https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms) sample database hosted on Azure. However, you can modify the prompts as needed to work with other database systems.

## Getting Started

These instructions will get you a copy of the sample up and running using PromptFlow for development and testing purposes.

### Prerequisites

1.  The libraries required to run this project are listed in the `requirements.txt` file. To install them, use the following command:

```bash
    pip  install  -r  requirements.txt
```

2. Deploy an Azure SQL Database with sample (AdventureWorksLT) - [Setup Guide)([Create a single database - Azure SQL Database | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-create-quickstart?view=azuresql&tabs=azure-portal)
3. Install and setup the [ODBC Driver for Microsoft SQL Server (17)](https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16#version-17) to connect to the Azure SQL Database using pyodbc
   - (Local PromptFlow): Install the driver on the development machine
   - (Azure AI Studio): The Automatic Runtime doesn't contain the ODBC Driver, so you need to create a custom runtime with promptflow, pyodbc, and the driver installed. [Customize Runtime](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/create-manage-runtime)
4. (Optional but highly recommended) Install and configure [Prompt flow for VS Code extension](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow) follow [Quick Start Guide](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html).

   ðŸ’¡ This extension is optional but highly recommended for flow development and debugging.

### Connections Setup

We need to setup 2 connections to run this sample. [Manage PromptFlow Connections](https://microsoft.github.io/promptflow/how-to-guides/manage-connections.html)

1.  Azure OpenAI connection: Create a connection to you Azure OpenAI endpoint with a GPT-4 Model deployment.
2.  Custom Connection: Create a custom connection for the Azure SQL Database. Sample YAML

```YML
$schema: https://azuremlschemas.azureedge.net/promptflow/latest/CustomConnection.schema.json
name: "to_replace_with_connection_name"
type: custom
configs:
  Server_name: "tcp:<server-name>.database.windows.net,1433"
  User_name: "<sql-login-name>"
  Database_name: "<database-name>"
secrets:
# Don't replace the '<user-input>' placeholder. The application will prompt you to enter a value when it runs.
  Password: "<user-input>"
```

**_NOTE:_**

- Ensure connectivity to your Azure SQL Database. Ref: https://learn.microsoft.com/en-us/azure/azure-sql/database/connectivity-settings?view=azuresql&tabs=azure-portal
- Change version of the ODBC driver in get_table_names.py and get_table_schema.py based on the version installed in your machine/runtime.

Once the connections are created, modify the python and LLM nodes in the flow.dag.yaml with the appropriate connections.

###### For Example -

###### get_table_names (python tool)

```YAML
- name: get_table_names
    type: python
    source:
    type: code
    path: get_table_names.py
    inputs:
    sqlconn: <sql-connection-name>
    schema_name: ${inputs.schema_name}
```

###### identify_table_llm (LLM tool)

```YAML
- name: identify_table_llm
  type: llm
  source:
    type: code
    path: identify_table_llm.jinja2
  inputs:
    deployment_name: gpt-4
    response_format:
    type: text
    user_query: ${inputs.user_query}
    table_names: ${get_table_names.output}
  connection: azure_openai_connection
  api: chat
```

#### Sample Flow Run (VSCode)

![Sample Flow Run](./media/flow_run_sample.png)

#### Batch Run

[test-inputs.jsonl](./test-inputs.jsonl) contains a list of sample inputs to execute a batch run against. You can use the outputs from the batch run as inputs to execute batch evaluation on a set of user queries.

### Important Notes

- This sample is not intended for production.
- Given approach only works when the Table and Column names in the Database are descriptive enough and can be mapped through the natural language query.
- Be descriptive in the ask. Explicitly call out the column names and relationships where possible to be able to generate accurate queries.

###### Credits - 
Sample queries - https://github.com/keilorg/sqlzoo-solutions/blob/main/16%20AdventureWorks.sql